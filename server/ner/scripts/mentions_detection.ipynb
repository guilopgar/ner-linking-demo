{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bsc-bio-ehr-es\"\n",
    "SEQ_LEN = 128\n",
    "LR = 3e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = \"../\"\n",
    "model_root_path = os.path.join(\n",
    "    ROOT_PATH,\n",
    "    \"models\"\n",
    ")\n",
    "\n",
    "text_path = \"../data/text.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-24 17:23:09.431339: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, XLMRobertaTokenizerFast, RobertaTokenizerFast\n",
    "\n",
    "# All variables that depend on model_name\n",
    "\n",
    "    \n",
    "if MODEL_NAME == 'bsc-bio-ehr-es':\n",
    "    model_path = os.path.join(\n",
    "        model_root_path,\n",
    "        \"RoBERTa/pytorch\",\n",
    "        MODEL_NAME\n",
    "    )\n",
    "    tokenizer = RobertaTokenizerFast.from_pretrained(model_path, do_lower_case=False)\n",
    "    \n",
    "else:\n",
    "    raise Exception(\"ERROR: NO AVAILABLE MODEL!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Auxiliary components\n",
    "import sys\n",
    "sys.path.append(\n",
    "    os.path.join(ROOT_PATH, \"src\")\n",
    ")\n",
    "\n",
    "import utils.ner.load_data as load_data\n",
    "import utils.ner.pre_process as pre_proc\n",
    "import utils.ner.post_process as post_proc\n",
    "import utils.tf.loss as tf_loss\n",
    "\n",
    "# Hyper-parameters\n",
    "text_col = \"raw_text\"\n",
    "GREEDY = True\n",
    "IGNORE_VALUE = -100\n",
    "LOGITS = False\n",
    "ROUND_N = 4\n",
    "\n",
    "custom_tokenizer = pre_proc.TransformersTokenizer(\n",
    "    tokenizer=tokenizer, ign_value=IGNORE_VALUE\n",
    ")\n",
    "\n",
    "# IOB labels\n",
    "B_VAL, I_VAL, EMPTY_VAL = \"B\", \"I\", \"O\"\n",
    "ALLOW_IN_AS_BEGIN = False\n",
    "\n",
    "custom_tokenizer = pre_proc.TransformersTokenizer(\n",
    "    tokenizer=tokenizer, ign_value=IGNORE_VALUE\n",
    ")\n",
    "\n",
    "random_seed = 0\n",
    "tf.random.set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_text = []\n",
    "with open(text_path, \"r\") as file:\n",
    "    arr_text.append(file.read())\n",
    "df_text = pd.DataFrame({'doc_id': [\"text\"], 'raw_text': arr_text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data pre-processing\n",
    "\n",
    "We generate the valid inputs to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label encoders as dict (more computationally efficient)\n",
    "lab_encoder = {B_VAL: 0, I_VAL: 1, EMPTY_VAL: 2}\n",
    "lab_decoder = {0: B_VAL, 1: I_VAL, 2: EMPTY_VAL}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the custom pre-processing objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_annotator = pre_proc.AnnotatorContinuous(\n",
    "    labeler=pre_proc.LabelerIOB(\n",
    "        empty_val=EMPTY_VAL,\n",
    "        begin_val=B_VAL,\n",
    "        inside_val=I_VAL\n",
    "    )\n",
    ")\n",
    "\n",
    "sub_lab_converter = pre_proc.AllSubLabel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = sorted(set(df_text[\"doc_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_empty = pd.DataFrame({\n",
    "    \"doc_id\": []\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_tok_dict, text_y, text_frag, text_start_end_frag, \\\n",
    "                text_word_id = pre_proc.create_input_data(df_text=df_text, text_col=text_col, \n",
    "                                    df_ann=df_empty,\n",
    "                                    arr_doc=doc_list, ss_dict=None,\n",
    "                                    tokenizer=custom_tokenizer, \n",
    "                                    arr_lab_encoder=[lab_encoder], \n",
    "                                    seq_len=SEQ_LEN,\n",
    "                                    annotator=custom_annotator,\n",
    "                                    sub_lab_converter=sub_lab_converter,\n",
    "                                    greedy=GREEDY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Exec time of pre-processing documents (in mins): 0.020047342777252196 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1. Exec time of pre-processing documents (in mins):\", (end_time - start_time) / 60, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ind, text_att = text_tok_dict['input_ids'], text_tok_dict['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here guille now: create for loop for different clinical entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_ent_type = [\"enfermedad\", \"procedimiento\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_type = arr_ent_type[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-24 17:26:10.398659: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2023-08-24 17:26:10.430654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-24 17:26:10.431141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.6575GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2023-08-24 17:26:10.431163: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-08-24 17:26:10.434079: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2023-08-24 17:26:10.434121: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-08-24 17:26:10.435038: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2023-08-24 17:26:10.435242: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2023-08-24 17:26:10.436233: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2023-08-24 17:26:10.437005: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-08-24 17:26:10.437127: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-08-24 17:26:10.437213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-24 17:26:10.437891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-24 17:26:10.438343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2023-08-24 17:26:10.438687: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-24 17:26:10.439690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-24 17:26:10.440311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.6575GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2023-08-24 17:26:10.440385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-24 17:26:10.440877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-24 17:26:10.441315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2023-08-24 17:26:10.441345: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-08-24 17:26:10.935115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-08-24 17:26:10.935141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2023-08-24 17:26:10.935149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2023-08-24 17:26:10.935318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-24 17:26:10.935684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-24 17:26:10.936008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-24 17:26:10.936338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10306 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "2023-08-24 17:26:11.410210: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2023-08-24 17:26:11.654605: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForTokenClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForTokenClassification, TFXLMRobertaForTokenClassification, TFRobertaForTokenClassification \n",
    "\n",
    "if MODEL_NAME.split('_')[0] in ('beto', 'mbert'):\n",
    "    model = TFBertForTokenClassification.from_pretrained(model_path, from_pt=True)\n",
    "    \n",
    "elif MODEL_NAME.split('_')[0] == 'xlmr':\n",
    "    model = TFXLMRobertaForTokenClassification.from_pretrained(model_path, from_pt=True)\n",
    "\n",
    "elif MODEL_NAME in ('bsc-bio-ehr-es', 'roberta-base-bne', 'roberta-large-bne'):\n",
    "    model = TFRobertaForTokenClassification.from_pretrained(model_path, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home_ext/guillermo/miniconda3/envs/tf25_picasso/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "\n",
    "iob_num_labels = len(lab_encoder)\n",
    "\n",
    "input_ids = Input(shape=(SEQ_LEN,), name='input_ids', dtype='int64')\n",
    "attention_mask = Input(shape=(SEQ_LEN,), name='attention_mask', dtype='int64')\n",
    "\n",
    "out_seq = model.layers[0](input_ids=input_ids, attention_mask=attention_mask)[0] # take the output sub-token sequence \n",
    "\n",
    "# IOB-2\n",
    "out_iob = Dense(units=iob_num_labels, kernel_initializer=GlorotUniform(seed=random_seed))(out_seq) # Multi-class classification \n",
    "out_iob_model = Activation(activation='softmax', name='iob_output')(out_iob)\n",
    "\n",
    "model = Model(inputs=[input_ids, attention_mask], outputs=out_iob_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "roberta (TFRobertaMainLayer)    TFBaseModelOutputWit 124052736   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128, 3)       2307        roberta[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "iob_output (Activation)         (None, 128, 3)       0           dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 124,055,043\n",
      "Trainable params: 124,055,043\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fa66074eac0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\"\"\"\n",
    "optimizer = tfa.optimizers.RectifiedAdam(learning_rate=LR)\n",
    "loss = {'iob_output': tf_loss.TokenClassificationLoss(\n",
    "    from_logits=LOGITS, ignore_val=IGNORE_VALUE\n",
    ")}\n",
    "loss_weights = {'iob_output': 1}\n",
    "model.compile(optimizer=optimizer, loss=loss, loss_weights=loss_weights)\n",
    "\"\"\"\n",
    "# Load model weights\n",
    "model.load_weights(\n",
    "    os.path.join(\n",
    "        model_root_path,\n",
    "        \"model_checkpoints\",\n",
    "        entity_type\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-24 17:35:17.608131: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2023-08-24 17:35:17.628702: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 3199980000 Hz\n"
     ]
    }
   ],
   "source": [
    "text_preds = model.predict({'input_ids': text_ind, 'attention_mask': text_att})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Exec time of making predictions (in mins): 0.09016862710316977\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2. Exec time of making predictions (in mins):\", (end_time - start_time) / 60, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data post-processing\n",
    "\n",
    "We post-process the models predictions to generate valid annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the custom post-processing objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_preds_converter = post_proc.ProdWordPreds()\n",
    "\n",
    "custom_ann_extractor = post_proc.AnnExtractorContinuous(\n",
    "    lab_extractor=post_proc.LabExtractorIOB(\n",
    "        arr_lab_decoder=[lab_decoder],\n",
    "        empty_val=EMPTY_VAL,\n",
    "        begin_val=B_VAL,\n",
    "        inside_val=I_VAL\n",
    "    ), \n",
    "    allow_inside_as_begin=ALLOW_IN_AS_BEGIN\n",
    ")\n",
    "\n",
    "custom_preds_frag_tok = post_proc.NeuralPredsFragTok(\n",
    "    tokenizer=custom_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_codes = set(map(lambda k: k.split('\\t')[0], open(codes_path).readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtask = 'ner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_test = post_proc.extract_annotations_from_model_preds(arr_doc=test_doc_list, arr_frags=test_frag,\n",
    "                                      arr_preds=[test_preds],\n",
    "                                      arr_start_end=test_start_end_frag, arr_word_id=test_word_id,\n",
    "                                      arr_preds_pos_tok=custom_preds_frag_tok.calculate_pos_tok(\n",
    "                                          arr_len=test_start_end_frag\n",
    "                                      ),\n",
    "                                      ann_extractor=custom_ann_extractor,\n",
    "                                      word_preds_converter=word_preds_converter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_test = metrics.format_distemist_preds(\n",
    "    df_preds=df_pred_test,\n",
    "    df_text=df_text_test,\n",
    "    text_col=text_col\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "According to file headers, you are on subtask ner, predictions file\n"
     ]
    }
   ],
   "source": [
    "df_pred_test = metrics.format_distemist_df(df=df_pred_test, valid_codes=valid_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preds\n",
    "df_pred_test.to_csv(\n",
    "    RES_DIR + \"df_pred_test.tsv\", header=True, index=False, sep='\\t'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n3. Exec time of post-processing predictions (in mins):\", (end_time - start_time) / 60, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "According to file headers, you are on subtask ner, GS file\n"
     ]
    }
   ],
   "source": [
    "df_test_gs = metrics.format_distemist_df(\n",
    "    df=pd.read_csv(\n",
    "        corpus_path + \"test_annotated/\" + subtask_path.replace('training', 'test'), \n",
    "        header=0, sep=\"\\t\"\n",
    "    ),\n",
    "    valid_codes=valid_codes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f1 = metrics.calculate_distemist_metrics(gs=df_test_gs, pred=df_pred_test, subtask=subtask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-avg P = 0.8049 | micro-avg R = 0.764 | micro-avg F1 = 0.784\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nmicro-avg P = \", round(p, ROUND_N), \" | micro-avg R = \", round(r, ROUND_N), \n",
    "      \" | micro-avg F1 = \", round(f1, ROUND_N), \"\\n\", sep=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
